# ğŸ¯ æ•°å­¦å»ºæ¨¡ç®—æ³•å‚è€ƒæ‰‹å†Œ

> **å®šä½**ï¼šé›¶åŸºç¡€ä¹Ÿèƒ½è¯»æ‡‚çš„ç¾èµ›/å›½èµ›ç®—æ³•é€ŸæŸ¥æ‰‹å†Œ
> 
> **æ ¸å¿ƒç†å¿µ**ï¼šæ¨¡å‹æ˜¯"è¯´æœå·¥å…·"ï¼Œä¸æ˜¯"ç­”æ¡ˆæœºå™¨"ã€‚è¯„å§”çœ‹çš„æ˜¯ä½ çš„é€»è¾‘ï¼Œä¸æ˜¯ä½ ç®—å¾—å‡†ä¸å‡†ã€‚
å¯ä»¥æŸ¥çœ‹ä¸€ä¸‹**data_analysis/preprocessing/2025Cç¤ºä¾‹**é‡Œé¢çš„æ¨¡å‹åˆ†ææ–‡ä»¶å¤¹
---

## ğŸ“‹ ç›®å½•

1. [æ¨¡å‹é€‰æ‹©å†³ç­–æ ‘](#ä¸€æ¨¡å‹é€‰æ‹©å†³ç­–æ ‘)
2. [è¯„ä¼°æŒ‡æ ‡å¤§å…¨](#äºŒè¯„ä¼°æŒ‡æ ‡å¤§å…¨)
3. [é¢„æµ‹ç±»æ¨¡å‹](#ä¸‰é¢„æµ‹ç±»æ¨¡å‹)
4. [è¯„ä»·å†³ç­–ç±»æ¨¡å‹](#å››è¯„ä»·å†³ç­–ç±»æ¨¡å‹)
5. [åˆ†ç±»ä¸èšç±»æ¨¡å‹](#äº”åˆ†ç±»ä¸èšç±»æ¨¡å‹)
6. [ä¼˜åŒ–ç±»æ¨¡å‹](#å…­ä¼˜åŒ–ç±»æ¨¡å‹)
7. [ç»Ÿè®¡åˆ†æç±»æ¨¡å‹](#ä¸ƒç»Ÿè®¡åˆ†æç±»æ¨¡å‹)
8. [å›¾è®ºä¸ç½‘ç»œæ¨¡å‹](#å…«å›¾è®ºä¸ç½‘ç»œæ¨¡å‹)
9. [ä»¿çœŸç±»æ¨¡å‹](#ä¹ä»¿çœŸç±»æ¨¡å‹)

---

# ä¸€ã€æ¨¡å‹é€‰æ‹©å†³ç­–æ ‘

> ğŸ’¡ **æ‹¿åˆ°é¢˜ç›®ç¬¬ä¸€æ­¥ï¼šåˆ¤æ–­é—®é¢˜ç±»å‹**

## 1.1 æ€»å†³ç­–æ ‘

```
æ‹¿åˆ°é—®é¢˜åï¼Œé—®è‡ªå·±ï¼š
â”‚
â”œâ”€ è¦é¢„æµ‹æœªæ¥æ•°å€¼ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€é¢„æµ‹ç±»ã€‘
â”‚   â”œâ”€ æœ‰å¤šä¸ªå½±å“å› ç´ ï¼Ÿ â†’ å›å½’æ¨¡å‹ï¼ˆçº¿æ€§/Ridge/Lasso/XGBoostï¼‰
â”‚   â”œâ”€ åªæœ‰æ—¶é—´åºåˆ—ï¼Ÿ â†’ ARIMA / Prophet / æŒ‡æ•°å¹³æ»‘ / LSTM
â”‚   â”œâ”€ æ•°æ®å¾ˆå°‘(<15ä¸ª)ï¼Ÿ â†’ ç°è‰²é¢„æµ‹ GM(1,1)
â”‚   â”œâ”€ éçº¿æ€§å¾ˆå¼ºï¼Ÿ â†’ éšæœºæ£®æ— / XGBoost / GBDT
â”‚   â””â”€ éœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Ÿ â†’ MCMC / Bootstrap
â”‚
â”œâ”€ è¦è¯„ä»·/æ’åº/é€‰æ–¹æ¡ˆï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€è¯„ä»·å†³ç­–ç±»ã€‘
â”‚   â”œâ”€ éœ€è¦å®šæƒé‡ï¼Ÿ â†’ AHPï¼ˆä¸»è§‚ï¼‰/ ç†µæƒæ³•ï¼ˆå®¢è§‚ï¼‰
â”‚   â”œâ”€ æ–¹æ¡ˆæ’åºï¼Ÿ â†’ TOPSIS / PCA-TOPSIS
â”‚   â”œâ”€ æŒ‡æ ‡æ¨¡ç³Šï¼ˆå¥½/ä¸­/å·®ï¼‰ï¼Ÿ â†’ æ¨¡ç³Šç»¼åˆè¯„ä»·
â”‚   â””â”€ è¯„ä»·æ•ˆç‡ï¼Ÿ â†’ DEA
â”‚
â”œâ”€ è¦åˆ†ç±»/åˆ†ç¾¤ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€åˆ†ç±»èšç±»ç±»ã€‘
â”‚   â”œâ”€ æœ‰æ ‡ç­¾ï¼ˆçŸ¥é“ç­”æ¡ˆï¼‰ï¼Ÿ â†’ éšæœºæ£®æ— / SVM / å†³ç­–æ ‘ / Logisticå›å½’
â”‚   â”œâ”€ æ— æ ‡ç­¾ï¼ˆè‡ªåŠ¨åˆ†ç¾¤ï¼‰ï¼Ÿ â†’ K-means / å±‚æ¬¡èšç±» / DBSCAN
â”‚   â””â”€ å›¾åƒåˆ†ç±»ï¼Ÿ â†’ CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰
â”‚
â”œâ”€ è¦ä¼˜åŒ–ï¼ˆæ±‚æœ€å¤§/æœ€å°ï¼‰ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€ä¼˜åŒ–ç±»ã€‘
â”‚   â”œâ”€ çº¿æ€§çº¦æŸï¼Ÿ â†’ çº¿æ€§è§„åˆ’
â”‚   â”œâ”€ éçº¿æ€§/å¤æ‚ï¼Ÿ â†’ é—ä¼ ç®—æ³• / æ¨¡æ‹Ÿé€€ç«
â”‚   â”œâ”€ å¤šç›®æ ‡å†²çªï¼Ÿ â†’ NSGA-II / åŠ æƒå’Œæ³•
â”‚   â””â”€ åºè´¯å†³ç­–ï¼Ÿ â†’ åŠ¨æ€è§„åˆ’
â”‚
â”œâ”€ è¦åˆ†æå˜é‡å…³ç³»ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€ç»Ÿè®¡åˆ†æç±»ã€‘
â”‚   â”œâ”€ ä¸¤å˜é‡ç›¸å…³ï¼Ÿ â†’ ç›¸å…³åˆ†æï¼ˆPearson/Spearmanï¼‰
â”‚   â”œâ”€ å¤šç»„æ¯”è¾ƒï¼Ÿ â†’ æ–¹å·®åˆ†æ ANOVA / tæ£€éªŒ
â”‚   â”œâ”€ ç‰¹å¾é‡è¦æ€§ï¼Ÿ â†’ SHAPå€¼åˆ†æ / ç‰¹å¾é‡è¦æ€§æ’åº
â”‚   â””â”€ å…³è”è§„åˆ™ï¼Ÿ â†’ Apriori / FP-Growth
â”‚
â”œâ”€ è¦å¤„ç†æ–‡æœ¬æ•°æ®ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€NLPç±»ã€‘
â”‚   â”œâ”€ æƒ…æ„Ÿåˆ†æï¼Ÿ â†’ VADER / TextBlob / BERT
â”‚   â”œâ”€ ä¸»é¢˜æå–ï¼Ÿ â†’ LDA / TF-IDF
â”‚   â””â”€ å…³é”®è¯æå–ï¼Ÿ â†’ TF-IDF / TextRank
â”‚
â”œâ”€ è¦å»ºæ¨¡çŠ¶æ€è½¬ç§»ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€åºåˆ—/çŠ¶æ€ç±»ã€‘
â”‚   â”œâ”€ çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Ÿ â†’ é©¬å°”å¯å¤«é“¾
â”‚   â”œâ”€ éšè—çŠ¶æ€ï¼Ÿ â†’ éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰
â”‚   â””â”€ æ—¶åºä¾èµ–ï¼Ÿ â†’ LSTM / GRU
â”‚
â””â”€ è¦æ¨¡æ‹Ÿç³»ç»Ÿæ¼”åŒ–ï¼Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ã€ä»¿çœŸç±»ã€‘
    â”œâ”€ ä¸ç¡®å®šæ€§/é£é™©ï¼Ÿ â†’ è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
    â”œâ”€ ç©ºé—´æ‰©æ•£/æ¼”åŒ–ï¼Ÿ â†’ å…ƒèƒè‡ªåŠ¨æœº
    â””â”€ é£é™©åº¦é‡ï¼Ÿ â†’ CVaR / VaR
```

## 1.2 ç¾èµ›å¸¸è§"ç‹ç‚¸ç»„åˆ"

| é—®é¢˜ç±»å‹ | æ¨èç»„åˆ | è¯´æ˜ |
|----------|----------|------|
| ç»¼åˆè¯„ä»· | **AHP + TOPSIS** | AHPå®šæƒé‡ï¼ŒTOPSISæ’åº |
| é¢„æµ‹+ä¸ç¡®å®šæ€§ | **XGBoost + Bootstrap/MCMC** | é¢„æµ‹+ç½®ä¿¡åŒºé—´ |
| æ–¹æ¡ˆä¼˜é€‰ | **AHP + ç†µæƒæ³• + TOPSIS** | ä¸»å®¢è§‚ç»“åˆæ›´æœ‰è¯´æœåŠ› |
| é£é™©è¯„ä¼° | **Logisticå›å½’ + è’™ç‰¹å¡æ´›** | æ¦‚ç‡+æ¨¡æ‹Ÿ |
| æ–‡æœ¬åˆ†æ | **TF-IDF + LDA + æƒ…æ„Ÿåˆ†æ** | ç‰¹å¾æå–+ä¸»é¢˜+æƒ…æ„Ÿ |
| æ—¶ç©ºé¢„æµ‹ | **ARIMA/LSTM + ç©ºé—´èšç±»** | æ—¶é—´+ç©ºé—´ç»´åº¦ |
| æŠ•èµ„ä¼˜åŒ– | **é¢„æµ‹æ¨¡å‹ + åŠ¨æ€è§„åˆ’/é—ä¼ ç®—æ³•** | é¢„æµ‹+ä¼˜åŒ– |
| çŠ¶æ€å»ºæ¨¡ | **HMM/é©¬å°”å¯å¤« + éšæœºæ£®æ—** | çŠ¶æ€+é¢„æµ‹ |
| æ•ˆåº”åˆ†æ | **DID/å›å½’ + SHAPå€¼åˆ†æ** | å› æœ+è§£é‡Š |

---

## 1.3 ç¾èµ›Cé¢˜å†å¹´è€ƒå¯Ÿæ€»ç»“ï¼ˆ2020-2025ï¼‰

> ğŸ’¡ **Cé¢˜ç‰¹ç‚¹**ï¼šæ•°æ®é©±åŠ¨å‹ï¼Œå¼ºè°ƒæ•°æ®åˆ†æã€é¢„æµ‹å»ºæ¨¡ã€ä¸ç¡®å®šæ€§é‡åŒ–

| å¹´ä»½ | é¢˜ç›®ä¸»é¢˜ | æ ¸å¿ƒè€ƒå¯Ÿç‚¹ | æ¨èç®—æ³•æ¨¡å‹ |
|------|----------|------------|-------------|
| **2020** | äºšé©¬é€Šäº§å“è¯„è®ºåˆ†æ | æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æŒ–æ˜ã€äº§å“å£°èª‰é¢„æµ‹ | VADER/TextBlobï¼ˆæƒ…æ„Ÿåˆ†æï¼‰ã€TF-IDF+LDAï¼ˆæ–‡æœ¬ç‰¹å¾ï¼‰ã€æœ‰åºLogisticå›å½’ã€ARIMA/Prophetï¼ˆæ—¶åºï¼‰ã€éšæœºæ£®æ— |
| **2021** | äºšæ´²å·¨èœ‚å…¥ä¾µé¢„æµ‹ | æ—¶ç©ºä¼ æ’­é¢„æµ‹ã€å›¾åƒåˆ†ç±»ã€ä¼˜å…ˆçº§è¯„ä»· | ARIMA/LSTMï¼ˆæ—¶ç©ºé¢„æµ‹ï¼‰ã€CNNï¼ˆå›¾åƒåˆ†ç±»ï¼‰ã€SVM/å†³ç­–æ ‘ï¼ˆæŠ¥å‘Šåˆ†ç±»ï¼‰ã€åŠ æƒç»¼åˆè¯„ä»·ã€K-meansï¼ˆç©ºé—´èšç±»ï¼‰ |
| **2022** | é»„é‡‘æ¯”ç‰¹å¸äº¤æ˜“ç­–ç•¥ | ä»·æ ¼é¢„æµ‹ã€æŠ•èµ„ç»„åˆä¼˜åŒ–ã€é£é™©åº¦é‡ | ARIMA/XGBoostï¼ˆä»·æ ¼é¢„æµ‹ï¼‰ã€åŠ¨æ€è§„åˆ’/é—ä¼ ç®—æ³•ï¼ˆä¼˜åŒ–ï¼‰ã€CVaRï¼ˆé£é™©åº¦é‡ï¼‰ã€NSGA-IIï¼ˆå¤šç›®æ ‡ä¼˜åŒ–ï¼‰ã€æ•æ„Ÿæ€§åˆ†æ |
| **2023** | Wordleæ¸¸æˆç»“æœé¢„æµ‹ | æ—¶åºé¢„æµ‹ã€åˆ†å¸ƒé¢„æµ‹ã€éš¾åº¦åˆ†ç±» | äºŒæ¬¡æŒ‡æ•°å¹³æ»‘/ç°è‰²é¢„æµ‹ï¼ˆæ—¶åºï¼‰ã€éšæœºæ£®æ—/GBDTï¼ˆåˆ†å¸ƒé¢„æµ‹ï¼‰ã€K-means/å±‚æ¬¡èšç±»ï¼ˆéš¾åº¦åˆ†ç±»ï¼‰ã€çš®å°”é€Šç›¸å…³åˆ†æ |
| **2024** | ç½‘çƒæ¯”èµ›åŠ¿å¤´åˆ†æ | çŠ¶æ€å»ºæ¨¡ã€åŠ¿å¤´é¢„æµ‹ã€å¯è§†åŒ– | HMM/é©¬å°”å¯å¤«é“¾ï¼ˆçŠ¶æ€å»ºæ¨¡ï¼‰ã€éšæœºæ£®æ—/XGBoostï¼ˆé¢„æµ‹ï¼‰ã€Logisticå›å½’ï¼ˆèƒœè´Ÿï¼‰ã€PCA-TOPSISï¼ˆè¡¨ç°è¯„ä¼°ï¼‰ã€LSTM |
| **2025** | å¥¥è¿å¥–ç‰Œé¢„æµ‹ | å¥–ç‰Œé¢„æµ‹ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€æ•™ç»ƒæ•ˆåº” | éšæœºæ£®æ—/XGBoostï¼ˆé¢„æµ‹ï¼‰ã€MCMC/Bootstrapï¼ˆä¸ç¡®å®šæ€§ï¼‰ã€Logisticå›å½’ï¼ˆé¦–å¥–æ¦‚ç‡ï¼‰ã€SHAPå€¼ï¼ˆæ•ˆåº”åˆ†æï¼‰ã€å…³è”è§„åˆ™ |

### Cé¢˜é«˜é¢‘è€ƒå¯Ÿèƒ½åŠ›ç»Ÿè®¡

| èƒ½åŠ›ç±»å‹ | å‡ºç°é¢‘ç‡ | å…¸å‹æ¨¡å‹ | åº”ç”¨åœºæ™¯ |
|----------|----------|----------|----------|
| **æ—¶åºé¢„æµ‹** | â­â­â­â­â­ | ARIMAã€LSTMã€æŒ‡æ•°å¹³æ»‘ã€Prophet | ä»·æ ¼ã€æ•°é‡ã€è¶‹åŠ¿é¢„æµ‹ |
| **åˆ†ç±»/å›å½’é¢„æµ‹** | â­â­â­â­â­ | éšæœºæ£®æ—ã€XGBoostã€Logisticå›å½’ | ç»“æœé¢„æµ‹ã€æ¦‚ç‡ä¼°è®¡ |
| **ä¸ç¡®å®šæ€§é‡åŒ–** | â­â­â­â­ | Bootstrapã€MCMCã€ç½®ä¿¡åŒºé—´ | é¢„æµ‹åŒºé—´ã€é£é™©è¯„ä¼° |
| **ç‰¹å¾é‡è¦æ€§åˆ†æ** | â­â­â­â­ | SHAPã€ç‰¹å¾é‡è¦æ€§ã€ç›¸å…³åˆ†æ | å½±å“å› ç´ è¯†åˆ« |
| **èšç±»/åˆ†ç¾¤** | â­â­â­ | K-meansã€å±‚æ¬¡èšç±»ã€DBSCAN | åˆ†ç±»ã€åˆ†ç»„ã€å¼‚å¸¸æ£€æµ‹ |
| **æ–‡æœ¬/NLPåˆ†æ** | â­â­â­ | TF-IDFã€LDAã€æƒ…æ„Ÿåˆ†æ | è¯„è®ºåˆ†æã€ä¸»é¢˜æå– |
| **ä¼˜åŒ–å†³ç­–** | â­â­â­ | åŠ¨æ€è§„åˆ’ã€é—ä¼ ç®—æ³•ã€çº¿æ€§è§„åˆ’ | ç­–ç•¥ä¼˜åŒ–ã€èµ„æºé…ç½® |
| **çŠ¶æ€/åºåˆ—å»ºæ¨¡** | â­â­ | HMMã€é©¬å°”å¯å¤«é“¾ã€LSTM | çŠ¶æ€è½¬ç§»ã€åºåˆ—é¢„æµ‹ |
| **ç»¼åˆè¯„ä»·** | â­â­ | AHPã€TOPSISã€ç†µæƒæ³• | æ–¹æ¡ˆæ’åºã€å¤šæŒ‡æ ‡è¯„ä»· |

---

## 1.4 æ‰©å±•æ¨¡å‹é€ŸæŸ¥

### æ–‡æœ¬åˆ†æç±»ï¼ˆNLPï¼‰

| æ¨¡å‹ | ç”¨é€” | Pythonåº“ |
|------|------|----------|
| **VADER** | è‹±æ–‡æƒ…æ„Ÿåˆ†æï¼ˆç¤¾äº¤åª’ä½“ï¼‰ | `nltk.sentiment.vader` |
| **TextBlob** | ç®€å•æƒ…æ„Ÿåˆ†æ | `textblob` |
| **TF-IDF** | æ–‡æœ¬ç‰¹å¾æå–ã€å…³é”®è¯ | `sklearn.feature_extraction.text` |
| **LDA** | ä¸»é¢˜å»ºæ¨¡ | `gensim` æˆ– `sklearn` |
| **Word2Vec** | è¯å‘é‡è¡¨ç¤º | `gensim` |

### æ—¶åºé¢„æµ‹ç±»

| æ¨¡å‹ | é€‚ç”¨åœºæ™¯ | Pythonåº“ |
|------|----------|----------|
| **ARIMA** | å¹³ç¨³æ—¶åºã€çŸ­æœŸé¢„æµ‹ | `statsmodels` |
| **SARIMA** | å¸¦å­£èŠ‚æ€§çš„æ—¶åº | `statsmodels` |
| **Prophet** | å¸¦è¶‹åŠ¿+å­£èŠ‚çš„æ—¶åº | `prophet` |
| **LSTM/GRU** | å¤æ‚éçº¿æ€§æ—¶åº | `tensorflow/keras` |
| **æŒ‡æ•°å¹³æ»‘** | ç®€å•è¶‹åŠ¿é¢„æµ‹ | `statsmodels` |

### ä¼˜åŒ–ç±»

| æ¨¡å‹ | é€‚ç”¨åœºæ™¯ | Pythonåº“ |
|------|----------|----------|
| **çº¿æ€§è§„åˆ’** | çº¿æ€§ç›®æ ‡+çº¿æ€§çº¦æŸ | `scipy.optimize.linprog` |
| **æ•´æ•°è§„åˆ’** | å˜é‡å¿…é¡»ä¸ºæ•´æ•° | `scipy.optimize.milp` |
| **é—ä¼ ç®—æ³•** | å¤æ‚éçº¿æ€§ä¼˜åŒ– | `deap` æˆ– `scipy.optimize` |
| **åŠ¨æ€è§„åˆ’** | åºè´¯å†³ç­–é—®é¢˜ | è‡ªè¡Œå®ç° |
| **NSGA-II** | å¤šç›®æ ‡ä¼˜åŒ– | `pymoo` |

### é£é™©åº¦é‡ç±»

| æŒ‡æ ‡ | å«ä¹‰ | åº”ç”¨ |
|------|------|------|
| **VaR** | é£é™©ä»·å€¼ï¼Œæœ€å¤§å¯èƒ½æŸå¤± | é‡‘èé£é™© |
| **CVaR** | æ¡ä»¶é£é™©ä»·å€¼ï¼Œå°¾éƒ¨é£é™© | æç«¯æƒ…å†µè¯„ä¼° |
| **å¤æ™®æ¯”ç‡** | é£é™©è°ƒæ•´åæ”¶ç›Š | æŠ•èµ„ç»„åˆè¯„ä»· |

---

# äºŒã€è¯„ä¼°æŒ‡æ ‡å¤§å…¨

> ğŸ’¡ **æ¨¡å‹ä¸æ˜¯"ç®—å‡ºæ¥"çš„ï¼Œæ˜¯"ç”¨æŒ‡æ ‡è¯æ˜å‡ºæ¥"çš„**

## 2.1 å›å½’/é¢„æµ‹æ¨¡å‹æŒ‡æ ‡

### RÂ²ï¼ˆå†³å®šç³»æ•°ï¼‰â­â­â­â­â­

**ä¸€å¥è¯**ï¼šæ¨¡å‹è§£é‡Šäº†å¤šå°‘æ•°æ®æ³¢åŠ¨

```
èŒƒå›´ï¼š0 â‰¤ RÂ² â‰¤ 1
```

| RÂ² å€¼ | å«ä¹‰ |
|-------|------|
| â‰ˆ 0 | å‡ ä¹æ²¡å­¦åˆ°è§„å¾‹ |
| 0.3~0.6 | æœ‰ä¸€å®šè§£é‡Šèƒ½åŠ› |
| 0.6~0.8 | æ‹Ÿåˆè¾ƒå¥½ |
| > 0.8 | éå¸¸å¥½ï¼ˆè­¦æƒ•è¿‡æ‹Ÿåˆï¼‰|

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The coefficient of determination (RÂ²=0.85) indicates that the model explains 85% of the variance in medal counts.

**ğŸ¯ 2025Cé¢˜åº”ç”¨**ï¼š
> åœ¨å¥–ç‰Œé¢„æµ‹æ¨¡å‹ä¸­ï¼ŒLassoå›å½’çš„RÂ²=0.9484ï¼Œè¯´æ˜æ¨¡å‹èƒ½è§£é‡Š94.84%çš„å¥–ç‰Œæ•°å˜åŒ–ã€‚

---

### MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰â­â­â­â­â­

**ä¸€å¥è¯**ï¼šé¢„æµ‹å€¼å’ŒçœŸå®å€¼å¹³å‡å·®å¤šå°‘

```python
MAE = mean(|y_true - y_pred|)
```

- å•ä½ä¸åŸæ•°æ®ä¸€è‡´
- ä¸å¤¸å¤§æç«¯è¯¯å·®
- **è®ºæ–‡å‹å¥½å‹æŒ‡æ ‡**

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The Mean Absolute Error (MAE=3.2) suggests that the average prediction deviation is approximately 3 medals.

---

### RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰â­â­â­â­â­

**ä¸€å¥è¯**ï¼šå¯¹"å¤§é”™"æƒ©ç½šæ›´é‡çš„è¯¯å·®æŒ‡æ ‡

```python
RMSE = sqrt(mean((y_true - y_pred)Â²))
```

| MAE vs RMSE | ç‰¹ç‚¹ |
|-------------|------|
| MAE | äººäººå¹³ç­‰ |
| RMSE | é”™å¾—ç¦»è°±ä¼šè¢«é‡ç‚¹æƒ©ç½š |

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> RMSE is adopted to penalize large deviations, with a value of 5.8 indicating acceptable prediction accuracy.

---

### MAPEï¼ˆå¹³å‡ç›¸å¯¹è¯¯å·®ï¼‰â­â­â­â­

**ä¸€å¥è¯**ï¼šå¹³å‡è¯¯å·®å çœŸå®å€¼çš„ç™¾åˆ†æ¯”

```python
MAPE = mean(|y_true - y_pred| / y_true) Ã— 100%
```

âš ï¸ **æ³¨æ„**ï¼šçœŸå®å€¼ä¸èƒ½æœ‰0

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The MAPE of 8.5% demonstrates that the model achieves satisfactory relative accuracy.

---

### æŒ‡æ ‡ç»„åˆå»ºè®®

| æ¨¡å‹ç±»å‹ | æ¨èæŒ‡æ ‡ç»„åˆ |
|----------|--------------|
| çº¿æ€§å›å½’ | RÂ² + MAE + RMSE |
| æ—¶é—´åºåˆ— | RMSE + MAPE + AIC |
| ç°è‰²é¢„æµ‹ | å¹³å‡ç›¸å¯¹è¯¯å·® + åéªŒå·®æ¯”C |
| ç¥ç»ç½‘ç»œ | RÂ² + MAE + RMSE + Lossæ›²çº¿ |

---

## 2.2 åˆ†ç±»æ¨¡å‹æŒ‡æ ‡

### æ··æ·†çŸ©é˜µ â­â­â­â­â­

**å¿…ç”»ï¼æ‰€æœ‰åˆ†ç±»æŒ‡æ ‡çš„åŸºç¡€**

|  | é¢„æµ‹=æ­£ | é¢„æµ‹=è´Ÿ |
|--|---------|---------|
| **å®é™…=æ­£** | TPï¼ˆçœŸæ­£ä¾‹ï¼‰| FNï¼ˆæ¼æŠ¥ï¼‰|
| **å®é™…=è´Ÿ** | FPï¼ˆè¯¯æŠ¥ï¼‰| TNï¼ˆçœŸè´Ÿä¾‹ï¼‰|

---

### Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰

```python
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

âš ï¸ **é™·é˜±**ï¼šç±»åˆ«ä¸å¹³è¡¡æ—¶ä¼šéª—äººï¼

---

### Precision / Recall / F1 â­â­â­â­â­

| æŒ‡æ ‡ | å…¬å¼ | å«ä¹‰ |
|------|------|------|
| Precision | TP/(TP+FP) | é¢„æµ‹ä¸ºæ­£çš„æœ‰å¤šå‡† |
| Recall | TP/(TP+FN) | å®é™…ä¸ºæ­£çš„æ‰¾å›å¤šå°‘ |
| F1 | 2Ã—PÃ—R/(P+R) | ä¸¤è€…çš„è°ƒå’Œå¹³å‡ |

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The model achieves a Precision of 0.89 and Recall of 0.92, with an F1-score of 0.90, indicating balanced classification performance.

---

### AUC-ROC â­â­â­â­â­

**ä¸€å¥è¯**ï¼šåˆ†ç±»æ¨¡å‹çš„"RÂ²"

```
èŒƒå›´ï¼š0.5 â‰¤ AUC â‰¤ 1.0
```

| AUCå€¼ | å«ä¹‰ |
|-------|------|
| 0.5 | éšæœºçŒœæµ‹ |
| 0.7~0.8 | ä¸€èˆ¬ |
| 0.8~0.9 | è‰¯å¥½ |
| > 0.9 | ä¼˜ç§€ |

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The AUC of 0.87 demonstrates strong discriminative ability of the classifier.

---

## 2.3 èšç±»æ¨¡å‹æŒ‡æ ‡

### è½®å»“ç³»æ•°ï¼ˆSilhouetteï¼‰â­â­â­â­â­

**ä¸€å¥è¯**ï¼šèšç±»ç‰ˆçš„"RÂ²"

```
èŒƒå›´ï¼š-1 â‰¤ s â‰¤ 1
```

| å€¼ | å«ä¹‰ |
|----|------|
| æ¥è¿‘1 | èšå¾—å¾ˆå¥½ |
| â‰ˆ 0 | ç±»åˆ«é‡å  |
| < 0 | åˆ†é”™äº† |

---

### è‚˜éƒ¨æ³•åˆ™ï¼ˆElbow Methodï¼‰â­â­â­â­â­

**ç”¨äºç¡®å®šKå€¼**ï¼šçœ‹SSEéšKå˜åŒ–çš„"æ‹ç‚¹"

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> Based on the elbow method, K=4 is selected where the rate of SSE decrease significantly slows down.

---

## 2.4 æ—¶é—´åºåˆ—æ¨¡å‹æŒ‡æ ‡

### AIC / BIC

**ä¸€å¥è¯**ï¼šåœ¨"æ‹Ÿåˆå¥½"å’Œ"åˆ«å¤ªå¤æ‚"ä¹‹é—´æ‰¾å¹³è¡¡

```
AIC / BIC è¶Šå°ï¼Œæ¨¡å‹è¶Šå¥½
```

**ğŸ“ è®ºæ–‡å¥å¼**ï¼š
> The ARIMA(1,1,1) model is selected based on the minimum AIC value of 256.3.

---

## 2.5 æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æ¨¡å‹ç±»å‹ | æ ¸å¿ƒæŒ‡æ ‡ | å¯¹æ ‡ |
|----------|----------|------|
| å›å½’ | RÂ², RMSE, MAE | RÂ²è¶Šå¤§è¶Šå¥½ |
| åˆ†ç±» | AUC, F1 | AUCâ‰ˆåˆ†ç±»ç‰ˆRÂ² |
| èšç±» | è½®å»“ç³»æ•° | è½®å»“â‰ˆèšç±»ç‰ˆRÂ² |
| æ—¶é—´åºåˆ— | AIC, MAPE | AICè¶Šå°è¶Šå¥½ |

---

# ä¸‰ã€é¢„æµ‹ç±»æ¨¡å‹

## 3.1 çº¿æ€§å›å½’ï¼ˆåŸºç¡€å¿…ä¼šï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­â­

### é€‚ç”¨åœºæ™¯
- å¤šå› ç´ å½±å“ä¸€ä¸ªæ•°å€¼ç»“æœ
- éœ€è¦åˆ†æå„å› ç´ çš„å½±å“æ–¹å‘å’Œå¼ºåº¦
- éœ€è¦é«˜å¯è§£é‡Šæ€§

### æ ¸å¿ƒæ€æƒ³
> å‡è®¾ä¸–ç•Œæ˜¯"å¤§è‡´çº¿æ€§çš„"ï¼šåŸå› å˜ä¸€ç‚¹ï¼Œç»“æœä¹Ÿè·Ÿç€å˜ä¸€ç‚¹

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# æŸ¥çœ‹ç³»æ•°ï¼ˆé‡è¦ï¼ï¼‰
print("å„ç‰¹å¾å½±å“æƒé‡:", model.coef_)
print("æˆªè·:", model.intercept_)
```

### è¯„ä»·æŒ‡æ ‡
```python
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
```

### ğŸ“ è®ºæ–‡å¥å¼

> A multiple linear regression model is established to quantify the influence of key factors on medal counts. The regression coefficients indicate that historical performance (Î²=0.85) has the most significant positive impact.

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> åœ¨å¥¥è¿å¥–ç‰Œé¢„æµ‹ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹ï¼Œä»¥`total_lag1`ã€`is_host`ç­‰ä¸ºè‡ªå˜é‡é¢„æµ‹`Total`ã€‚å›å½’ç³»æ•°æ˜¾ç¤ºï¼Œ`total_lag1`(Î²=0.82)å½±å“æœ€å¤§ï¼ŒéªŒè¯äº†"å†å²è¡¨ç°æ˜¯æœ€å¼ºé¢„æµ‹å› å­"çš„å‡è®¾ã€‚

---

## 3.2 æ­£åˆ™åŒ–å›å½’ï¼ˆRidge / Lassoï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- ç‰¹å¾è¾ƒå¤šï¼Œæ‹…å¿ƒè¿‡æ‹Ÿåˆ
- ç‰¹å¾é—´å­˜åœ¨å¤šé‡å…±çº¿æ€§
- éœ€è¦è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼ˆLassoï¼‰

### æ ¸å¿ƒåŒºåˆ«

| æ–¹æ³• | æ­£åˆ™åŒ– | ç‰¹ç‚¹ |
|------|--------|------|
| Ridge | L2 | ç³»æ•°ç¼©å°ä½†ä¸ä¸º0 |
| Lasso | L1 | ç³»æ•°å¯èƒ½å˜ä¸º0ï¼ˆè‡ªåŠ¨é€‰ç‰¹å¾ï¼‰|

```python
from sklearn.linear_model import Ridge, Lasso

# Ridgeå›å½’
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lassoå›å½’ï¼ˆä¼šè‡ªåŠ¨å‰”é™¤ä¸é‡è¦ç‰¹å¾ï¼‰
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

### ğŸ“ è®ºæ–‡å¥å¼

> Lasso regression is employed to address multicollinearity and perform automatic feature selection. The L1 regularization drives less important coefficients to zero, yielding a more parsimonious model.

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> ç”±äº`total_lag1`ã€`gold_lag1`ã€`total_rolling3_mean`å­˜åœ¨é«˜åº¦å…±çº¿æ€§ï¼ˆç›¸å…³ç³»æ•°>0.8ï¼‰ï¼Œæˆ‘ä»¬é‡‡ç”¨Lassoå›å½’è¿›è¡Œç‰¹å¾ç­›é€‰ã€‚ç»“æœæ˜¾ç¤ºLasso(RÂ²=0.9484)ç•¥ä¼˜äºæ™®é€šçº¿æ€§å›å½’(RÂ²=0.9454)ã€‚

---

## 3.3 ARIMAæ—¶é—´åºåˆ—

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­â­

### é€‚ç”¨åœºæ™¯
- åªæœ‰å†å²æ—¶é—´æ•°æ®
- æ•°æ®æŒ‰æ—¶é—´æ’åˆ—ä¸”æœ‰è§„å¾‹
- éœ€è¦é¢„æµ‹æœªæ¥è¶‹åŠ¿

### æ ¸å¿ƒæ€æƒ³
> ä»Šå¤© â‰ˆ æ˜¨å¤© + å‰å¤© + éšæœºæ³¢åŠ¨

### ARIMA(p,d,q)å‚æ•°
- **p**ï¼šè‡ªå›å½’é˜¶æ•°ï¼ˆçœ‹PACFå›¾ï¼‰
- **d**ï¼šå·®åˆ†æ¬¡æ•°ï¼ˆè®©æ•°æ®å¹³ç¨³ï¼‰
- **q**ï¼šç§»åŠ¨å¹³å‡é˜¶æ•°ï¼ˆçœ‹ACFå›¾ï¼‰

```python
from statsmodels.tsa.arima.model import ARIMA

# å»ºç«‹æ¨¡å‹
model = ARIMA(data, order=(1, 1, 1))
fitted = model.fit()

# é¢„æµ‹æœªæ¥10æœŸ
forecast = fitted.forecast(steps=10)

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
print(fitted.summary())
print(f"AIC: {fitted.aic}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> An ARIMA(1,1,1) model is constructed based on the Box-Jenkins methodology. The model selection is guided by the minimum AIC criterion, and the Ljung-Box test confirms that residuals exhibit no significant autocorrelation.

---

## 3.4 ç°è‰²é¢„æµ‹ GM(1,1)

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- æ•°æ®ç‚¹å¾ˆå°‘ï¼ˆ4~15ä¸ªï¼‰
- æ•°æ®å‘ˆå•è°ƒè¶‹åŠ¿
- ä¸æ»¡è¶³ç»Ÿè®¡å­¦æ ·æœ¬é‡è¦æ±‚

### æ ¸å¿ƒæ€æƒ³
> æŠŠæ•°æ®ç´¯åŠ å¹³æ»‘ï¼Œå‡è®¾æŒ‰æŒ‡æ•°å‘å±•

```python
def grey_model_gm11(x0, n_predict=3):
    """
    GM(1,1)ç°è‰²é¢„æµ‹æ¨¡å‹
    x0: åŸå§‹åºåˆ—
    n_predict: é¢„æµ‹æ­¥æ•°
    """
    n = len(x0)
    x1 = np.cumsum(x0)  # ä¸€æ¬¡ç´¯åŠ ç”Ÿæˆ
    
    # æ„é€ æ•°æ®çŸ©é˜µ
    B = np.zeros((n-1, 2))
    Y = x0[1:].reshape((n-1, 1))
    
    for i in range(n-1):
        B[i][0] = -(x1[i] + x1[i+1]) / 2
        B[i][1] = 1
    
    # æœ€å°äºŒä¹˜ä¼°è®¡å‚æ•°
    params = np.linalg.inv(B.T @ B) @ B.T @ Y
    a, b = params[0][0], params[1][0]
    
    # é¢„æµ‹
    predictions = []
    for k in range(1, n + n_predict + 1):
        x1_pred = (x0[0] - b/a) * np.exp(-a * (k-1)) + b/a
        predictions.append(x1_pred)
    
    # ç´¯å‡è¿˜åŸ
    x0_pred = np.diff(predictions)
    x0_pred = np.insert(x0_pred, 0, predictions[0])
    
    return x0_pred[:n], x0_pred[n:]  # æ‹Ÿåˆå€¼, é¢„æµ‹å€¼
```

### è¯„ä»·æŒ‡æ ‡
- **å¹³å‡ç›¸å¯¹è¯¯å·®**ï¼š< 10% ä¸ºåˆæ ¼
- **åéªŒå·®æ¯”C**ï¼š< 0.35 ä¸ºä¼˜ç§€
- **å°è¯¯å·®æ¦‚ç‡P**ï¼š> 0.95 ä¸ºä¼˜ç§€

### ğŸ“ è®ºæ–‡å¥å¼

> Given the limited historical data (n=8), a GM(1,1) grey prediction model is applied. The posterior variance ratio C=0.28 and small error probability P=0.98 indicate excellent model fitting.

---

## 3.5 Logisticå›å½’ï¼ˆé¢„æµ‹æ¦‚ç‡ï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- é¢„æµ‹"æ˜¯/å¦"ã€"æˆåŠŸ/å¤±è´¥"
- è¾“å‡ºæ¦‚ç‡è€Œéç¡®å®šå€¼
- é£é™©è¯„ä¼°ã€äº‹ä»¶å‘ç”Ÿå¯èƒ½æ€§

### æ ¸å¿ƒæ€æƒ³
> æŠŠå„ç§å› ç´ çš„å½±å“ï¼Œè½¬åŒ–ä¸ºäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡(0~1)

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹æ¦‚ç‡
probabilities = model.predict_proba(X_test)[:, 1]

# é¢„æµ‹ç±»åˆ«
predictions = model.predict(X_test)
```

### ğŸ“ è®ºæ–‡å¥å¼

> Logistic regression is employed to estimate the probability of a country winning more than 50 medals. The model yields an AUC of 0.89, demonstrating strong predictive power for identifying potential medal powerhouses.

---

## 3.6 éšæœºæ£®æ—å›å½’

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- ç‰¹å¾å¤šã€å…³ç³»å¤æ‚
- ä¸ç¡®å®šæ˜¯å¦çº¿æ€§
- éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ

### æ ¸å¿ƒæ€æƒ³
> å¾ˆå¤š"ä¸å¤ªèªæ˜"çš„æ ‘ï¼ŒæŠ•ç¥¨/å¹³å‡å†³å®šç»“æœ

```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# ç‰¹å¾é‡è¦æ€§ï¼ˆç¾èµ›åŠ åˆ†é¡¹ï¼ï¼‰
importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Random Forest model with 100 trees is trained to capture non-linear relationships. Feature importance analysis reveals that historical medal count contributes 45% to the prediction, followed by host status (18%).

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> å°½ç®¡éšæœºæ£®æ—(RÂ²=0.917)ç•¥ä½äºçº¿æ€§æ¨¡å‹(RÂ²=0.948)ï¼Œä½†å…¶ç‰¹å¾é‡è¦æ€§åˆ†æç›´è§‚å±•ç¤ºäº†`total_lag1`(45%)å’Œ`is_host`(18%)æ˜¯æœ€é‡è¦çš„é¢„æµ‹å› å­ï¼Œä¸ç›¸å…³æ€§åˆ†æç»“è®ºä¸€è‡´ã€‚

---

# å››ã€è¯„ä»·å†³ç­–ç±»æ¨¡å‹

## 4.1 AHPï¼ˆå±‚æ¬¡åˆ†ææ³•ï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­â­

### é€‚ç”¨åœºæ™¯
- å¤šå‡†åˆ™å†³ç­–é—®é¢˜
- æƒé‡ä¸æ˜ç¡®ï¼Œéœ€è¦"ä¸“å®¶åˆ¤æ–­"
- å®šæ€§+å®šé‡æŒ‡æ ‡æ··åˆ

### æ ¸å¿ƒæ€æƒ³
> ä¸¤ä¸¤æ¯”è¾ƒï¼šå“ªä¸ªæ›´é‡è¦ï¼Ÿé‡è¦å¤šå°‘ï¼Ÿ

### åˆ¤æ–­çŸ©é˜µæ ‡åº¦

| æ ‡åº¦ | å«ä¹‰ |
|------|------|
| 1 | åŒç­‰é‡è¦ |
| 3 | ç¨å¾®é‡è¦ |
| 5 | æ˜æ˜¾é‡è¦ |
| 7 | å¼ºçƒˆé‡è¦ |
| 9 | æç«¯é‡è¦ |

```python
import numpy as np

def ahp_weights(matrix):
    """è®¡ç®—AHPæƒé‡"""
    n = matrix.shape[0]
    
    # æ–¹æ³•1ï¼šå‡ ä½•å¹³å‡æ³•
    row_product = np.prod(matrix, axis=1)
    row_geo_mean = row_product ** (1/n)
    weights = row_geo_mean / row_geo_mean.sum()
    
    # ä¸€è‡´æ€§æ£€éªŒ
    lambda_max = np.sum(matrix @ weights / weights) / n
    CI = (lambda_max - n) / (n - 1)
    RI = [0, 0, 0.58, 0.90, 1.12, 1.24, 1.32, 1.41, 1.45]  # éšæœºä¸€è‡´æ€§æŒ‡æ ‡
    CR = CI / RI[n-1] if n > 2 else 0
    
    return weights, CR

# ç¤ºä¾‹ï¼šæ¯”è¾ƒ3ä¸ªå› ç´ çš„é‡è¦æ€§
matrix = np.array([
    [1,   3,   5],    # å› ç´ 1
    [1/3, 1,   2],    # å› ç´ 2  
    [1/5, 1/2, 1]     # å› ç´ 3
])

weights, cr = ahp_weights(matrix)
print(f"æƒé‡: {weights}")
print(f"ä¸€è‡´æ€§æ¯”ç‡CR: {cr:.4f} {'(é€šè¿‡)' if cr < 0.1 else '(éœ€è°ƒæ•´)'}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> The Analytic Hierarchy Process (AHP) is employed to determine indicator weights through pairwise comparison. The consistency ratio CR=0.05<0.1 confirms the reliability of the judgment matrix.

---

## 4.2 ç†µæƒæ³•ï¼ˆå®¢è§‚èµ‹æƒï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­â­

### é€‚ç”¨åœºæ™¯
- éœ€è¦å®¢è§‚æƒé‡ï¼ˆä¸ä¾èµ–ä¸»è§‚åˆ¤æ–­ï¼‰
- æœ‰å¤šä¸ªè¯„ä»·æŒ‡æ ‡
- å¸¸ä¸AHPç»„åˆä½¿ç”¨

### æ ¸å¿ƒæ€æƒ³
> ä¿¡æ¯ç†µè¶Šå°ï¼ˆæ•°æ®å·®å¼‚è¶Šå¤§ï¼‰ï¼ŒæŒ‡æ ‡è¶Šé‡è¦

```python
def entropy_weight(data):
    """ç†µæƒæ³•è®¡ç®—æƒé‡"""
    # æ ‡å‡†åŒ–
    data_norm = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0) + 1e-10)
    
    # è®¡ç®—æ¯”é‡
    p = data_norm / data_norm.sum(axis=0)
    p = np.where(p == 0, 1e-10, p)  # é¿å…log(0)
    
    # è®¡ç®—ç†µå€¼
    n = data.shape[0]
    e = -1/np.log(n) * np.sum(p * np.log(p), axis=0)
    
    # è®¡ç®—æƒé‡
    d = 1 - e  # å·®å¼‚ç³»æ•°
    weights = d / d.sum()
    
    return weights
```

### ğŸ“ è®ºæ–‡å¥å¼

> The Entropy Weight Method is applied to objectively determine indicator weights based on data variability. Higher weights are assigned to indicators with greater dispersion.

---

## 4.3 TOPSISï¼ˆæ’åºç¥å™¨ï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- å¤šå¯¹è±¡ç»¼åˆæ’å
- æ–¹æ¡ˆæ¯”è¾ƒä¼˜é€‰
- æ•°å€¼å‹æŒ‡æ ‡ä¸ºä¸»

### æ ¸å¿ƒæ€æƒ³
> å¥½æ–¹æ¡ˆ = ç¦»"æœ€å¥½"è¿‘ + ç¦»"æœ€å·®"è¿œ

```python
def topsis(data, weights, is_benefit):
    """
    TOPSISè¯„ä»·
    data: å†³ç­–çŸ©é˜µ (mä¸ªæ–¹æ¡ˆ Ã— nä¸ªæŒ‡æ ‡)
    weights: æŒ‡æ ‡æƒé‡
    is_benefit: æ˜¯å¦ä¸ºæ­£å‘æŒ‡æ ‡ï¼ˆTrue=è¶Šå¤§è¶Šå¥½ï¼‰
    """
    # æ ‡å‡†åŒ–
    norm_data = data / np.sqrt((data**2).sum(axis=0))
    
    # åŠ æƒ
    weighted = norm_data * weights
    
    # ç¡®å®šæ­£è´Ÿç†æƒ³è§£
    ideal_pos = np.where(is_benefit, weighted.max(axis=0), weighted.min(axis=0))
    ideal_neg = np.where(is_benefit, weighted.min(axis=0), weighted.max(axis=0))
    
    # è®¡ç®—è·ç¦»
    d_pos = np.sqrt(((weighted - ideal_pos)**2).sum(axis=1))
    d_neg = np.sqrt(((weighted - ideal_neg)**2).sum(axis=1))
    
    # è®¡ç®—è´´è¿‘åº¦
    scores = d_neg / (d_pos + d_neg)
    ranks = scores.argsort()[::-1] + 1
    
    return scores, ranks
```

### ğŸ“ è®ºæ–‡å¥å¼

> The TOPSIS method is employed to rank alternatives based on their relative closeness to the ideal solution. The results indicate that Alternative A achieves the highest score (0.78), followed by B (0.65) and C (0.52).

---

## 4.4 æ¨¡ç³Šç»¼åˆè¯„ä»·

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- ä¸»è§‚/æ¨¡ç³ŠæŒ‡æ ‡ï¼ˆæ»¡æ„åº¦ã€èˆ’é€‚åº¦ï¼‰
- åªèƒ½è¯´"å¥½/ä¸­/å·®"
- é—®å·è°ƒæŸ¥åˆ†æ

### æ ¸å¿ƒæ€æƒ³
> æŠŠ"æ¨¡ç³Šåˆ¤æ–­"é‡åŒ–ä¸ºéš¶å±åº¦

```python
def fuzzy_evaluation(weights, membership_matrix, levels):
    """
    æ¨¡ç³Šç»¼åˆè¯„ä»·
    weights: æŒ‡æ ‡æƒé‡
    membership_matrix: éš¶å±åº¦çŸ©é˜µ (æŒ‡æ ‡æ•° Ã— ç­‰çº§æ•°)
    levels: è¯„ä»·ç­‰çº§åˆ†å€¼
    """
    # æ¨¡ç³Šç»¼åˆè¿ç®—
    B = weights @ membership_matrix
    B = B / B.sum()  # å½’ä¸€åŒ–
    
    # è®¡ç®—ç»¼åˆå¾—åˆ†
    score = B @ np.array(levels)
    
    # ç¡®å®šç­‰çº§
    max_level_idx = B.argmax()
    
    return B, score, max_level_idx
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Fuzzy Comprehensive Evaluation model is constructed with evaluation grades {Excellent, Good, Medium, Poor}. The final membership vector B=(0.35, 0.42, 0.18, 0.05) indicates that the overall evaluation tends toward "Good".

---

## 4.5 DEAï¼ˆæ•°æ®åŒ…ç»œåˆ†æï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- è¯„ä»·"æ•ˆç‡"è€Œé"å¥½å"
- å¤šæŠ•å…¥å¤šäº§å‡ºç³»ç»Ÿ
- ä¸éœ€è¦é¢„è®¾æƒé‡

### æ ¸å¿ƒæ€æƒ³
> åˆ«äººç”¨æ›´å°‘èµ„æºï¼Œå¹²æ›´å¤šäº‹ï¼Œé‚£å°±æ›´æœ‰æ•ˆç‡

### ğŸ“ è®ºæ–‡å¥å¼

> Data Envelopment Analysis (DEA) is applied to evaluate the relative efficiency of decision-making units. The CCR model identifies 5 units as technically efficient (Î¸=1), while the remaining units exhibit varying degrees of inefficiency.

---

## 4.6 ä¸»å®¢è§‚ç»„åˆæƒé‡

**ç¾èµ›å¼ºçƒˆæ¨è**ï¼šâ­â­â­â­â­

```python
def combined_weight(subjective, objective, alpha=0.5):
    """
    ç»„åˆæƒé‡
    alpha: ä¸»è§‚æƒé‡çš„æ¯”ä¾‹
    """
    return alpha * subjective + (1 - alpha) * objective
```

### ğŸ“ è®ºæ–‡å¥å¼

> To balance subjectivity and objectivity, the final weights are obtained by combining AHP weights (subjective) and entropy weights (objective) with equal proportion (Î±=0.5).

---

# äº”ã€åˆ†ç±»ä¸èšç±»æ¨¡å‹

## 5.1 K-meansèšç±»

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- æ— æ ‡ç­¾ï¼Œæƒ³"åˆ†å †"
- å‘ç°æ•°æ®å†…éƒ¨ç»“æ„
- å®¢æˆ·åˆ†ç¾¤ã€åŒºåŸŸåˆ’åˆ†

### æ ¸å¿ƒæ€æƒ³
> æŠŠç‚¹åˆ†æˆKå †ï¼Œè®©æ¯å †å†…éƒ¨å°½é‡ç´§å‡‘

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šK
inertias = []
silhouettes = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(data, kmeans.labels_))

# æœ€ç»ˆèšç±»
best_k = 4  # æ ¹æ®è‚˜éƒ¨æ³•åˆ™ç¡®å®š
kmeans = KMeans(n_clusters=best_k, random_state=42)
labels = kmeans.fit_predict(data)
```

### ğŸ“ è®ºæ–‡å¥å¼

> K-means clustering is performed to segment countries into distinct groups. The optimal number of clusters (K=4) is determined by the elbow method, with a silhouette coefficient of 0.65 confirming good cluster separation.

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> æˆ‘ä»¬å¯¹å„å›½å†å²å¥–ç‰Œè¡¨ç°è¿›è¡ŒK-meansèšç±»ï¼Œè¯†åˆ«å‡º4ç±»å›½å®¶ï¼šè¶…çº§å¼ºå›½(ç¾ä¸­ä¿„)ã€ä¼ ç»Ÿå¼ºå›½(è‹±å¾·æ³•æ—¥)ã€ä¸­ç­‰æ°´å¹³å›½å®¶ã€ä»¥åŠå¶å°”è·å¥–å›½å®¶ã€‚è¿™ä¸€åˆ†ç±»ä¸ºåˆ†å±‚é¢„æµ‹æ¨¡å‹æä¾›äº†ä¾æ®ã€‚

---

## 5.2 éšæœºæ£®æ—åˆ†ç±»

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- æœ‰æ ‡ç­¾çš„åˆ†ç±»é—®é¢˜
- ç‰¹å¾å¤šã€å…³ç³»å¤æ‚
- éœ€è¦ç‰¹å¾é‡è¦æ€§

### æ ¸å¿ƒæ€æƒ³
> å¾ˆå¤šä¸å¤ªèªæ˜çš„æ ‘ï¼ŒæŠ•ç¥¨å†³å®šç±»åˆ«

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# é¢„æµ‹
y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)[:, 1]

# è¯„ä»·
print(classification_report(y_test, y_pred))
print(f"AUC: {roc_auc_score(y_test, y_prob):.4f}")

# ç‰¹å¾é‡è¦æ€§
importance = pd.Series(rf.feature_importances_, index=feature_names)
print(importance.sort_values(ascending=False))
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Random Forest classifier with 100 trees is trained to predict medal categories. The model achieves an accuracy of 0.87 and AUC of 0.92. Feature importance analysis highlights historical performance as the dominant predictor.

---

## 5.3 SVMï¼ˆæ”¯æŒå‘é‡æœºï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- æ ·æœ¬å°‘ã€ç»´åº¦é«˜
- è¾¹ç•Œæ¸…æ™°çš„åˆ†ç±»é—®é¢˜
- æ–‡æœ¬ã€å›¾åƒåˆ†ç±»

### æ ¸å¿ƒæ€æƒ³
> æ‰¾ä¸€æ¡åˆ†ç•Œçº¿ï¼Œè®©ä¸¤è¾¹ç¦»å¾—å°½é‡è¿œ

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# SVMå¯¹å°ºåº¦æ•æ„Ÿï¼Œå¿…é¡»æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è®­ç»ƒ
svm = SVC(kernel='rbf', probability=True)
svm.fit(X_train_scaled, y_train)
```

### ğŸ“ è®ºæ–‡å¥å¼

> Support Vector Machine with RBF kernel is employed given the high-dimensional feature space. The model achieves an F1-score of 0.85, demonstrating effective classification with limited samples.

---

## 5.4 å†³ç­–æ ‘

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- éœ€è¦"è§„åˆ™"ï¼Œä¸æ˜¯é»‘ç›’
- å¯è§£é‡Šæ€§è¦æ±‚é«˜
- ç”Ÿæˆå†³ç­–æµç¨‹å›¾

### æ ¸å¿ƒæ€æƒ³
> ä¸€æ­¥ä¸€æ­¥é—®é—®é¢˜ï¼ŒæŠŠæ ·æœ¬åˆ†å¼€

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree

dt = DecisionTreeClassifier(max_depth=4, random_state=42)
dt.fit(X_train, y_train)

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20, 10))
plot_tree(dt, feature_names=feature_names, class_names=class_names, filled=True)
plt.show()
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Decision Tree classifier is constructed for its interpretability. The tree structure reveals that if historical medals > 50 and is_host = 1, the country is predicted to achieve high performance with 92% confidence.

---

# å…­ã€ä¼˜åŒ–ç±»æ¨¡å‹

## 6.1 çº¿æ€§è§„åˆ’

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- èµ„æºåˆ†é…ã€ç”Ÿäº§è®¡åˆ’
- ç›®æ ‡å’Œçº¦æŸéƒ½æ˜¯çº¿æ€§çš„
- è¿½æ±‚æœ€å¤§/æœ€å°åŒ–

```python
from scipy.optimize import linprog

# ç›®æ ‡ï¼šæœ€å°åŒ– c'x
c = [1, 2]  # ç›®æ ‡å‡½æ•°ç³»æ•°

# ä¸ç­‰å¼çº¦æŸï¼šAx <= b
A_ub = [[-1, 1], [1, 2]]
b_ub = [1, 4]

# ç­‰å¼çº¦æŸï¼šA_eq x = b_eq
A_eq = [[1, 1]]
b_eq = [3]

# å˜é‡è¾¹ç•Œ
bounds = [(0, None), (0, None)]

# æ±‚è§£
result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)
print(f"æœ€ä¼˜è§£: {result.x}")
print(f"æœ€ä¼˜å€¼: {result.fun}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> A linear programming model is formulated to minimize total cost subject to resource constraints. The optimal solution allocates xâ‚=2 and xâ‚‚=1 with a minimum cost of 4 units.

---

## 6.2 æ•´æ•°è§„åˆ’

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- å˜é‡å¿…é¡»æ˜¯æ•´æ•°
- é€‰å€ã€è°ƒåº¦ã€åˆ†é…é—®é¢˜

```python
from scipy.optimize import milp, LinearConstraint, Bounds

# ç›®æ ‡å‡½æ•°ç³»æ•°
c = np.array([1, 2, 3])

# çº¦æŸ
constraints = LinearConstraint(A=[[1, 1, 1], [2, 1, 0]], lb=[3, 2], ub=[np.inf, np.inf])

# æ•´æ•°çº¦æŸ
integrality = np.ones(3)  # 1è¡¨ç¤ºæ•´æ•°ï¼Œ0è¡¨ç¤ºè¿ç»­

# æ±‚è§£
result = milp(c, constraints=constraints, integrality=integrality, bounds=Bounds(0, np.inf))
```

### ğŸ“ è®ºæ–‡å¥å¼

> An integer programming model is developed to determine the optimal facility locations. The binary decision variables represent whether to open a facility at each candidate site.

---

## 6.3 é—ä¼ ç®—æ³•

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- é—®é¢˜å¤æ‚ï¼Œæ— æ³•ç”¨ä¼ ç»Ÿæ–¹æ³•
- ç»„åˆä¼˜åŒ–ã€å‚æ•°è°ƒä¼˜
- å…¨å±€æœç´¢

### æ ¸å¿ƒæ€æƒ³
> æ¨¡æ‹Ÿè‡ªç„¶é€‰æ‹©ï¼šé€‰æ‹©ã€äº¤å‰ã€å˜å¼‚

```python
# ä½¿ç”¨ DEAP åº“
from deap import base, creator, tools, algorithms

# æˆ–ä½¿ç”¨ scipy
from scipy.optimize import differential_evolution

def objective(x):
    return x[0]**2 + x[1]**2

bounds = [(-5, 5), (-5, 5)]
result = differential_evolution(objective, bounds)
print(f"æœ€ä¼˜è§£: {result.x}")
print(f"æœ€ä¼˜å€¼: {result.fun}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Genetic Algorithm is employed to solve the complex optimization problem. After 100 generations with a population size of 50, the algorithm converges to a near-optimal solution with fitness value of 0.02.

---

## 6.4 å¤šç›®æ ‡è§„åˆ’

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- å¤šä¸ªç›®æ ‡ç›¸äº’å†²çª
- æ— æ³•åŒæ—¶æœ€ä¼˜åŒ–æ‰€æœ‰ç›®æ ‡
- éœ€è¦æƒè¡¡æŠ˜ä¸­

### æ ¸å¿ƒæ¦‚å¿µ
- **å¸•ç´¯æ‰˜å‰æ²¿**ï¼šæ”¹å–„ä¸€ä¸ªç›®æ ‡å¿…ç„¶ç‰ºç‰²å¦ä¸€ä¸ª
- **åŠ æƒæ³•**ï¼šç»™ç›®æ ‡åˆ†é…æƒé‡

```python
from scipy.optimize import minimize

def multi_objective(x, w1=0.5, w2=0.5):
    f1 = x[0]**2 + x[1]**2  # ç›®æ ‡1
    f2 = (x[0]-1)**2 + (x[1]-1)**2  # ç›®æ ‡2
    return w1 * f1 + w2 * f2  # åŠ æƒå’Œ

result = minimize(multi_objective, x0=[0, 0])
```

### ğŸ“ è®ºæ–‡å¥å¼

> A multi-objective optimization model is formulated to balance cost minimization and quality maximization. The Pareto front is generated using the Îµ-constraint method, providing decision-makers with a set of trade-off solutions.

---

# ä¸ƒã€ç»Ÿè®¡åˆ†æç±»æ¨¡å‹

## 7.1 ç›¸å…³åˆ†æ

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­â­

### Pearson vs Spearman

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | å…³ç³»ç±»å‹ |
|------|----------|----------|
| Pearson | è¿ç»­ã€æ­£æ€ã€çº¿æ€§ | çº¿æ€§å…³ç³» |
| Spearman | æœ‰åºã€éæ­£æ€ | å•è°ƒå…³ç³» |

```python
from scipy.stats import pearsonr, spearmanr

# Pearsonç›¸å…³
r, p_value = pearsonr(x, y)
print(f"Pearson r = {r:.4f}, p = {p_value:.4f}")

# Spearmanç›¸å…³
rho, p_value = spearmanr(x, y)
print(f"Spearman Ï = {rho:.4f}, p = {p_value:.4f}")
```

### ç›¸å…³ç³»æ•°è§£è¯»

| rå€¼ | å¼ºåº¦ |
|-----|------|
| 0.8~1.0 | æå¼º |
| 0.6~0.8 | å¼º |
| 0.4~0.6 | ä¸­ç­‰ |
| 0.2~0.4 | å¼± |
| 0~0.2 | æå¼± |

### ğŸ“ è®ºæ–‡å¥å¼

> Pearson correlation analysis reveals a strong positive relationship between historical performance and current medals (r=0.80, p<0.001), indicating that past success is a reliable predictor of future performance.

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºï¼Œ`total_lag1`ä¸`Total`çš„Pearsonç›¸å…³ç³»æ•°ä¸º0.80(p<0.001)ï¼Œè¯´æ˜ä¸Šå±Šå¥–ç‰Œæ•°æ˜¯é¢„æµ‹æœ¬å±Šæˆç»©çš„æœ€å¼ºæŒ‡æ ‡ã€‚`is_host`ä¸`Total`çš„ç›¸å…³ç³»æ•°ä¸º0.37ï¼Œè™½ç„¶ä¸­ç­‰ï¼Œä½†è€ƒè™‘åˆ°å®ƒæ˜¯äºŒå…ƒå˜é‡ï¼Œåœ¨tæ£€éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¸œé“ä¸»æ•ˆåº”ã€‚

---

## 7.2 æ–¹å·®åˆ†æï¼ˆANOVAï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- æ¯”è¾ƒå¤šç»„å‡å€¼æ˜¯å¦æœ‰å·®å¼‚
- å®éªŒè®¾è®¡ã€å¤„ç†æ•ˆæœæ¯”è¾ƒ

```python
from scipy.stats import f_oneway

# å•å› ç´ æ–¹å·®åˆ†æ
group1 = [23, 25, 27, 22, 24]
group2 = [31, 33, 35, 30, 32]
group3 = [18, 20, 22, 17, 19]

f_stat, p_value = f_oneway(group1, group2, group3)
print(f"F = {f_stat:.4f}, p = {p_value:.4f}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> One-way ANOVA is conducted to compare medal counts across different continental groups. The significant F-value (F=15.32, p<0.001) indicates that geographic region has a significant effect on Olympic performance.

### ğŸ¯ 2025Cé¢˜åº”ç”¨

> æ–¹å·®åˆ†ææ˜¾ç¤ºä¸œé“ä¸»ä¸éä¸œé“ä¸»çš„å¥–ç‰Œæ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚(F=45.6, p<0.001)ã€‚äº‹åæ£€éªŒè¿›ä¸€æ­¥ç¡®è®¤ä¸œé“ä¸»å¹³å‡è·å¾—68.2æšå¥–ç‰Œï¼Œæ˜¾è‘—é«˜äºéä¸œé“ä¸»çš„11.3æšã€‚

---

## 7.3 ç°è‰²å…³è”åˆ†æ

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- å°æ ·æœ¬ï¼ˆ<15ä¸ªæ•°æ®ç‚¹ï¼‰
- å› ç´ æ’åº
- ä¸è¦æ±‚æ•°æ®åˆ†å¸ƒ

### æ ¸å¿ƒæ€æƒ³
> å“ªä¸ªå› ç´ çš„æ›²çº¿å½¢çŠ¶å’Œç»“æœæ›²çº¿æœ€åƒ

```python
def grey_relational_analysis(reference, comparisons, rho=0.5):
    """
    ç°è‰²å…³è”åˆ†æ
    reference: å‚è€ƒåºåˆ—ï¼ˆå› å˜é‡ï¼‰
    comparisons: æ¯”è¾ƒåºåˆ—ï¼ˆå„å› ç´ ï¼‰
    rho: åˆ†è¾¨ç³»æ•°ï¼Œé€šå¸¸å–0.5
    """
    # åˆå€¼åŒ–å¤„ç†
    ref_norm = reference / reference[0]
    comp_norm = comparisons / comparisons[:, 0:1]
    
    # è®¡ç®—å·®åºåˆ—
    diff = np.abs(comp_norm - ref_norm)
    
    # æœ€å¤§å·®å’Œæœ€å°å·®
    diff_min = diff.min()
    diff_max = diff.max()
    
    # å…³è”ç³»æ•°
    xi = (diff_min + rho * diff_max) / (diff + rho * diff_max)
    
    # å…³è”åº¦
    gamma = xi.mean(axis=1)
    
    return gamma
```

### ğŸ“ è®ºæ–‡å¥å¼

> Grey Relational Analysis is applied given the limited data points (n=10). The results indicate that Factor A exhibits the highest grey relational grade (Î³=0.82), suggesting it has the strongest influence on the target variable.

---

# å…«ã€å›¾è®ºä¸ç½‘ç»œæ¨¡å‹

## 8.1 æœ€çŸ­è·¯å¾„ï¼ˆDijkstraï¼‰

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

```python
import networkx as nx

# åˆ›å»ºå›¾
G = nx.Graph()
G.add_weighted_edges_from([
    (1, 2, 4), (1, 3, 2), (2, 3, 1), 
    (2, 4, 5), (3, 4, 8)
])

# æœ€çŸ­è·¯å¾„
path = nx.shortest_path(G, source=1, target=4, weight='weight')
length = nx.shortest_path_length(G, source=1, target=4, weight='weight')

print(f"æœ€çŸ­è·¯å¾„: {path}")
print(f"æœ€çŸ­è·ç¦»: {length}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> Dijkstra's algorithm is applied to find the shortest path in the transportation network. The optimal route from node A to node E has a total distance of 15 km, passing through nodes B and D.

---

## 8.2 æœ€å¤§æµ

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

```python
# æœ€å¤§æµé—®é¢˜
G = nx.DiGraph()
G.add_edge('s', 'a', capacity=10)
G.add_edge('s', 'b', capacity=5)
G.add_edge('a', 't', capacity=7)
G.add_edge('b', 't', capacity=8)
G.add_edge('a', 'b', capacity=3)

flow_value, flow_dict = nx.maximum_flow(G, 's', 't')
print(f"æœ€å¤§æµ: {flow_value}")
```

### ğŸ“ è®ºæ–‡å¥å¼

> The maximum flow algorithm is applied to determine the network capacity. The maximum flow from source to sink is 12 units, with the bottleneck identified at edge (A, B).

---

# ä¹ã€ä»¿çœŸç±»æ¨¡å‹

## 9.1 è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­â­

### é€‚ç”¨åœºæ™¯
- ä¸ç¡®å®šæ€§åˆ†æ
- é£é™©è¯„ä¼°
- å¤æ‚ç³»ç»Ÿæ¨¡æ‹Ÿ

### æ ¸å¿ƒæ€æƒ³
> ç”¨å¤§é‡éšæœºè¯•éªŒï¼Œæ¨¡æ‹Ÿæ‰€æœ‰å¯èƒ½ç»“æœ

```python
import numpy as np

def monte_carlo_simulation(n_simulations=10000):
    """è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç¤ºä¾‹ï¼šä¼°ç®—é¡¹ç›®é£é™©"""
    
    results = []
    for _ in range(n_simulations):
        # éšæœºç”Ÿæˆå„å› ç´ 
        cost = np.random.normal(100, 10)      # æˆæœ¬ï¼šå‡å€¼100ï¼Œæ ‡å‡†å·®10
        time = np.random.uniform(5, 15)       # æ—¶é—´ï¼š5-15å¤©å‡åŒ€åˆ†å¸ƒ
        quality = np.random.triangular(0.7, 0.9, 1.0)  # è´¨é‡ï¼šä¸‰è§’åˆ†å¸ƒ
        
        # è®¡ç®—ç»“æœ
        outcome = cost * time / quality
        results.append(outcome)
    
    results = np.array(results)
    
    # ç»Ÿè®¡åˆ†æ
    print(f"å‡å€¼: {results.mean():.2f}")
    print(f"æ ‡å‡†å·®: {results.std():.2f}")
    print(f"95%ç½®ä¿¡åŒºé—´: [{np.percentile(results, 2.5):.2f}, {np.percentile(results, 97.5):.2f}]")
    print(f"è¶…è¿‡1500çš„æ¦‚ç‡: {(results > 1500).mean()*100:.2f}%")
    
    return results

results = monte_carlo_simulation()
```

### ğŸ“ è®ºæ–‡å¥å¼

> Monte Carlo simulation with 10,000 iterations is conducted to quantify uncertainty. The results show a mean outcome of 1,250 with a 95% confidence interval of [980, 1,580]. The probability of exceeding the threshold is estimated at 12.5%.

---

## 9.2 å…ƒèƒè‡ªåŠ¨æœº

**ç¾èµ›å‡ºç°ç‡**ï¼šâ­â­â­

### é€‚ç”¨åœºæ™¯
- ç©ºé—´æ‰©æ•£æ¨¡æ‹Ÿ
- ä¼ æŸ“ç—…ã€æ£®æ—ç«ç¾ã€åŸå¸‚æ‰©å¼ 
- å±€éƒ¨è§„åˆ™äº§ç”Ÿå…¨å±€æ¨¡å¼

### æ ¸å¿ƒæ€æƒ³
> æ¯ä¸ªæ ¼å­åªçœ‹"é‚»å±…"çŠ¶æ€æ¥å†³å®šä¸‹ä¸€æ—¶åˆ»

```python
def cellular_automaton(grid, steps=10):
    """
    ç®€å•å…ƒèƒè‡ªåŠ¨æœºç¤ºä¾‹ï¼ˆç±»ä¼¼ç”Ÿå‘½æ¸¸æˆï¼‰
    """
    rows, cols = grid.shape
    
    for _ in range(steps):
        new_grid = grid.copy()
        for i in range(1, rows-1):
            for j in range(1, cols-1):
                # ç»Ÿè®¡é‚»å±…
                neighbors = (grid[i-1:i+2, j-1:j+2].sum() - grid[i,j])
                
                # è§„åˆ™ï¼šé‚»å±…å¤ªå°‘æˆ–å¤ªå¤šåˆ™"æ­»äº¡"
                if grid[i,j] == 1:
                    if neighbors < 2 or neighbors > 3:
                        new_grid[i,j] = 0
                else:
                    if neighbors == 3:
                        new_grid[i,j] = 1
        
        grid = new_grid
    
    return grid
```

### ğŸ“ è®ºæ–‡å¥å¼

> A Cellular Automaton model is developed to simulate the spatial spread of the phenomenon. The local rules are defined based on neighborhood states, and the simulation reveals emergent patterns consistent with observed data.

---

## âœ… ç¾èµ›å»ºæ¨¡æŠ€å·§

1. **æ¨¡å‹ä¸è¦å¤ªå¤æ‚**ï¼šå¤æ‚æ¨¡å‹å®¹æ˜“æ­¢æ­¥Må¥–
2. **å¤šåšæ£€éªŒ**ï¼šçµæ•åº¦åˆ†æã€ç¨³å®šæ€§åˆ†æè¶Šå¤šè¶Šå¥½
3. **å¯è§†åŒ–é‡è¦**ï¼šå›¾è¡¨è¦ç²¾ç¾ã€æ¸…æ™°ã€ä¸“ä¸š
4. **å‡è®¾è¦å……åˆ†**ï¼šè¯´æ˜åˆç†æ€§å’Œå¿…è¦æ€§
5. **åˆ›æ–°å¯å®¹é”™**ï¼šæœ‰åˆ›æ–°å³ä½¿æœ‰å°é”™è¯¯ä¹Ÿå¯èƒ½è·å¥–

---


> ğŸ“ **æœ€åæé†’**ï¼š
> 
> è¿™ä»½æ‰‹å†Œæ˜¯"é€ŸæŸ¥å·¥å…·"ï¼Œä¸æ˜¯"ä¸‡èƒ½ç­”æ¡ˆ"ã€‚
> 
> çœŸæ­£çš„å»ºæ¨¡èƒ½åŠ›æ¥è‡ªäºï¼š**å¤šåšé¢˜ + å¤šæ€è€ƒ + å¤šæ€»ç»“**ã€‚
> 
> ç¥ä½ æ¯”èµ›é¡ºåˆ©ï¼ğŸ‰
