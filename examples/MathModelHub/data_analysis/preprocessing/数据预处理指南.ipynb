{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔧 数学建模数据预处理指南\n",
        "\n",
        "> **核心理念**：数据预处理不是机械的步骤，而是对数据的深度理解。每一步处理都应该有明确的业务逻辑支撑。\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 目录\n",
        "\n",
        "1. [万能Prompt模板](#1-万能prompt模板) - 发给AI，让它帮你生成定制化代码\n",
        "2. [预处理检查清单](#2-预处理检查清单) - 确保不遗漏关键步骤\n",
        "3. [常用代码片段](#3-常用代码片段) - 可直接复制使用\n",
        "\n",
        "---\n",
        "\n",
        "## 如何使用本指南？\n",
        "\n",
        "**推荐工作流程**：\n",
        "\n",
        "```\n",
        "1. 拿到数据后，先用【万能Prompt】发给AI\n",
        "   ↓\n",
        "2. AI生成定制化的预处理代码\n",
        "   ↓\n",
        "3. 对照【检查清单】确认没有遗漏\n",
        "   ↓\n",
        "4. 如需补充，从【代码片段】中复制\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. 万能Prompt模板\n",
        "\n",
        "## 🎯 核心Prompt（直接复制使用）\n",
        "\n",
        "将下面的Prompt复制，**替换【】中的内容**，然后发给AI：\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Role**: 你是一位精通数据科学的教练，擅长撰写可解释、可复现的Jupyter Notebook。\n",
        "\n",
        "**Background**:\n",
        "- **题目**: 【填写比赛题目名称和简要描述】\n",
        "- **数据文件**: 【列出所有数据文件名，如: data1.csv, data2.xlsx】\n",
        "- **核心需求**: 创建一个数据预处理的 `.ipynb` 文件，代码与Markdown说明比例约1:1\n",
        "\n",
        "**Task**:\n",
        "请按以下结构创建Notebook，每一步都要有**Markdown说明业务逻辑**：\n",
        "\n",
        "### 第一步：数据加载与初探\n",
        "1. 读取所有数据文件\n",
        "2. 用 `.info()` 和 `.head()` 查看结构\n",
        "3. 总结观察到的数据特点（类型、缺失、异常等）\n",
        "\n",
        "### 第二步：数据清洗\n",
        "1. **缺失值处理**：说明为什么选择删除/填充/插值\n",
        "2. **重复值处理**：检查并处理\n",
        "3. **数据类型转换**：确保类型正确\n",
        "4. **异常值处理**：识别并说明处理策略\n",
        "\n",
        "### 第三步：数据合并与关联\n",
        "1. 识别各表之间的关联键\n",
        "2. 说明选择哪种JOIN方式及原因\n",
        "3. 验证合并结果的正确性\n",
        "\n",
        "### 第四步：特征工程\n",
        "根据题目需求，考虑创建以下特征：\n",
        "- 时序特征（滞后值、滚动平均、变化率）\n",
        "- 分类编码（One-Hot、Label Encoding）\n",
        "- 数值变换（标准化、对数变换）\n",
        "- 业务衍生特征\n",
        "\n",
        "### 第五步：数据验证与保存\n",
        "1. 检查处理后数据的完整性\n",
        "2. 保存为CSV供后续建模使用\n",
        "3. 总结处理流程和关键决策\n",
        "\n",
        "**Output Requirements**:\n",
        "- 每段代码前必须有Markdown说明\"为什么这样做\"\n",
        "- 代码中包含清晰的注释\n",
        "- 在关键节点输出验证信息\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Prompt使用示例\n",
        "\n",
        "假设你拿到的是一道关于**电商销售预测**的题目，数据文件有 `orders.csv`, `products.csv`, `users.csv`：\n",
        "\n",
        "\n",
        "**Role**: 你是一位精通数据科学的教练...（同上）\n",
        "\n",
        "**Background**:\n",
        "- **题目**: 2025美赛D题《电商销售预测》，需要预测未来30天的销售额\n",
        "- **数据文件**: orders.csv（订单记录）, products.csv（商品信息）, users.csv（用户画像）\n",
        "- **核心需求**: 创建数据预处理notebook，为时序预测模型准备数据\n",
        "\n",
        "**Task**:\n",
        "...（同上）\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. 预处理检查清单\n",
        "\n",
        "在完成数据预处理后，对照以下清单确认没有遗漏：\n",
        "\n",
        "## ✅ 数据加载阶段\n",
        "\n",
        "| 检查项 | 状态 | 备注 |\n",
        "|--------|------|------|\n",
        "| 所有数据文件都已加载 | ☐ | |\n",
        "| 检查了文件编码（UTF-8/GBK/Latin-1） | ☐ | 中文数据常见GBK |\n",
        "| 查看了每个表的 `.info()` | ☐ | |\n",
        "| 查看了每个表的 `.head()` 和 `.tail()` | ☐ | |\n",
        "| 记录了各表的行数和列数 | ☐ | |\n",
        "\n",
        "## ✅ 数据质量阶段\n",
        "\n",
        "| 检查项 | 状态 | 备注 |\n",
        "|--------|------|------|\n",
        "| 检查了缺失值 `.isnull().sum()` | ☐ | |\n",
        "| 明确了缺失值的业务含义 | ☐ | 是\"未知\"还是\"不适用\"? |\n",
        "| 检查了重复值 `.duplicated()` | ☐ | |\n",
        "| 检查了数据类型是否正确 | ☐ | 日期是datetime? 数值是int/float? |\n",
        "| 识别了可能的异常值 | ☐ | 用箱线图或统计方法 |\n",
        "\n",
        "## ✅ 数据合并阶段\n",
        "\n",
        "| 检查项 | 状态 | 备注 |\n",
        "|--------|------|------|\n",
        "| 明确了各表之间的关联关系 | ☐ | 一对一? 一对多? 多对多? |\n",
        "| 选择了合适的JOIN方式 | ☐ | LEFT/RIGHT/INNER/OUTER |\n",
        "| 验证了合并后的行数是否合理 | ☐ | |\n",
        "| 检查了合并后的缺失值 | ☐ | JOIN可能产生新的NULL |\n",
        "\n",
        "## ✅ 特征工程阶段\n",
        "\n",
        "| 检查项 | 状态 | 备注 |\n",
        "|--------|------|------|\n",
        "| 创建了必要的时序特征 | ☐ | lag, rolling, diff |\n",
        "| 处理了分类变量 | ☐ | 编码或哑变量 |\n",
        "| 考虑了特征之间的交互 | ☐ | |\n",
        "| 检查了特征与目标的相关性 | ☐ | |\n",
        "| **避免了数据泄露** | ☐ | ⚠️ 最重要！ |\n",
        "\n",
        "## ✅ 最终验证阶段\n",
        "\n",
        "| 检查项 | 状态 | 备注 |\n",
        "|--------|------|------|\n",
        "| 无缺失值或已合理处理 | ☐ | |\n",
        "| 数据类型全部正确 | ☐ | |\n",
        "| 保存了处理后的数据 | ☐ | |\n",
        "| 记录了所有处理决策 | ☐ | 在Markdown中说明 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. 常用代码片段\n",
        "\n",
        "以下代码可以直接复制到你的Notebook中使用。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 标准导入与设置\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 标准导入\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 显示设置\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 200)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print(\"✅ 环境准备完成\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 数据加载（处理各种编码问题）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smart_read_csv(filepath, **kwargs):\n",
        "    \"\"\"\n",
        "    智能读取CSV，自动尝试不同编码\n",
        "    \"\"\"\n",
        "    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']\n",
        "    \n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, encoding=encoding, **kwargs)\n",
        "            print(f\"✅ 成功读取 {filepath} (编码: {encoding})\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 读取 {filepath} 失败: {e}\")\n",
        "            return None\n",
        "    \n",
        "    print(f\"❌ 所有编码尝试失败: {filepath}\")\n",
        "    return None\n",
        "\n",
        "# 使用示例\n",
        "# df = smart_read_csv('data.csv')\n",
        "\n",
        "# 常见编码问题的直接解决方案\n",
        "# df = pd.read_csv('data.csv', encoding='gbk')       # 中文Windows\n",
        "# df = pd.read_csv('data.csv', encoding='latin-1')   # 特殊字符\n",
        "# df = pd.read_csv('data.csv', encoding='utf-8-sig') # 带BOM的UTF-8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 数据质量快速检查\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_quality_report(df, name=\"DataFrame\"):\n",
        "    \"\"\"\n",
        "    生成数据质量报告\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"📊 数据质量报告: {name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 基本信息\n",
        "    print(f\"\\n📏 形状: {df.shape[0]} 行 × {df.shape[1]} 列\")\n",
        "    \n",
        "    # 缺失值\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df) * 100).round(2)\n",
        "    missing_df = pd.DataFrame({\n",
        "        '缺失数': missing,\n",
        "        '缺失率%': missing_pct\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['缺失数'] > 0].sort_values('缺失率%', ascending=False)\n",
        "    \n",
        "    if len(missing_df) > 0:\n",
        "        print(f\"\\n⚠️ 缺失值统计 ({len(missing_df)} 列有缺失):\")\n",
        "        print(missing_df)\n",
        "    else:\n",
        "        print(\"\\n✅ 无缺失值\")\n",
        "    \n",
        "    # 重复值\n",
        "    dup_count = df.duplicated().sum()\n",
        "    print(f\"\\n🔄 重复行数: {dup_count} ({dup_count/len(df)*100:.2f}%)\")\n",
        "    \n",
        "    # 数据类型\n",
        "    print(f\"\\n📋 数据类型分布:\")\n",
        "    print(df.dtypes.value_counts())\n",
        "    \n",
        "    # 数值列统计\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\n📈 数值列统计 ({len(numeric_cols)} 列):\")\n",
        "        print(df[numeric_cols].describe().round(2))\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# 使用示例\n",
        "# data_quality_report(df, \"奖牌数据\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 缺失值处理模板\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 缺失值处理策略 ==========\n",
        "\n",
        "# 策略1：删除（缺失率高或无法推断）\n",
        "# df = df.dropna(subset=['关键列'])           # 删除特定列有缺失的行\n",
        "# df = df.dropna(thresh=len(df.columns)*0.8)  # 保留至少80%非空的行\n",
        "\n",
        "# 策略2：填充固定值（业务逻辑明确）\n",
        "# df['col'] = df['col'].fillna(0)            # 填充0（如：未获奖=0枚）\n",
        "# df['col'] = df['col'].fillna('Unknown')    # 填充未知（分类变量）\n",
        "\n",
        "# 策略3：填充统计值（数值型）\n",
        "# df['col'] = df['col'].fillna(df['col'].mean())    # 均值\n",
        "# df['col'] = df['col'].fillna(df['col'].median())  # 中位数（对异常值鲁棒）\n",
        "# df['col'] = df['col'].fillna(df['col'].mode()[0]) # 众数\n",
        "\n",
        "# 策略4：分组填充（考虑分组差异）\n",
        "# df['col'] = df.groupby('group_col')['col'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# 策略5：插值（时序数据）\n",
        "# df['col'] = df['col'].interpolate(method='linear')   # 线性插值\n",
        "# df['col'] = df['col'].interpolate(method='time')     # 时间插值（需要datetime索引）\n",
        "\n",
        "# 策略6：前向/后向填充（时序数据）\n",
        "# df['col'] = df['col'].fillna(method='ffill')  # 用前一个值填充\n",
        "# df['col'] = df['col'].fillna(method='bfill')  # 用后一个值填充\n",
        "\n",
        "print(\"📝 缺失值处理策略已加载，请根据业务逻辑选择合适的方法\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 特征工程常用代码\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 时序特征 ==========\n",
        "\n",
        "# 滞后特征（Lag Features）- 捕捉历史表现\n",
        "# ⚠️ 使用shift(1)获取上一期，shift(-1)会造成数据泄露！\n",
        "# df['value_lag1'] = df.groupby('entity')['value'].shift(1)\n",
        "# df['value_lag2'] = df.groupby('entity')['value'].shift(2)\n",
        "\n",
        "# 滚动特征（Rolling Features）- 平滑波动\n",
        "# df['value_rolling3'] = df.groupby('entity')['value'].transform(\n",
        "#     lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)  # shift(1)避免泄露\n",
        "# )\n",
        "\n",
        "# 变化率特征\n",
        "# df['value_change'] = df['value'] - df['value_lag1']\n",
        "# df['value_pct_change'] = df.groupby('entity')['value'].pct_change()\n",
        "\n",
        "# ========== 分类编码 ==========\n",
        "\n",
        "# One-Hot编码（类别少时使用）\n",
        "# df = pd.get_dummies(df, columns=['category_col'], prefix='cat')\n",
        "\n",
        "# Label编码（类别多或有序时使用）\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# df['category_encoded'] = le.fit_transform(df['category_col'])\n",
        "\n",
        "# ========== 数值变换 ==========\n",
        "\n",
        "# 标准化（均值0，标准差1）\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# df['value_scaled'] = scaler.fit_transform(df[['value']])\n",
        "\n",
        "# 归一化（0-1范围）\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# df['value_normalized'] = scaler.fit_transform(df[['value']])\n",
        "\n",
        "# 对数变换（处理右偏分布）\n",
        "# df['value_log'] = np.log1p(df['value'])  # log1p处理0值\n",
        "\n",
        "# ========== 交互特征 ==========\n",
        "\n",
        "# df['feature_interaction'] = df['feature1'] * df['feature2']\n",
        "# df['feature_ratio'] = df['feature1'] / df['feature2'].replace(0, np.nan)\n",
        "\n",
        "print(\"📝 特征工程模板已加载\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 数据合并与验证\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 合并方式选择指南 ==========\n",
        "\n",
        "# LEFT JOIN: 保留左表所有记录，右表匹配不上的填NULL\n",
        "# 适用场景：事实表为主，补充维度信息\n",
        "# df_result = pd.merge(df_main, df_dim, on='key', how='left')\n",
        "\n",
        "# INNER JOIN: 只保留两表都有的记录\n",
        "# 适用场景：只分析完整数据\n",
        "# df_result = pd.merge(df1, df2, on='key', how='inner')\n",
        "\n",
        "# OUTER JOIN: 保留两表所有记录\n",
        "# 适用场景：需要看全貌，找出不匹配的记录\n",
        "# df_result = pd.merge(df1, df2, on='key', how='outer')\n",
        "\n",
        "# ========== 合并验证函数 ==========\n",
        "\n",
        "def validate_merge(df_left, df_right, df_result, key, merge_type='left'):\n",
        "    \"\"\"\n",
        "    验证合并结果是否符合预期\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"🔍 合并验证 ({merge_type.upper()} JOIN on '{key}')\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    print(f\"\\n左表行数: {len(df_left)}\")\n",
        "    print(f\"右表行数: {len(df_right)}\")\n",
        "    print(f\"结果行数: {len(df_result)}\")\n",
        "    \n",
        "    # 检查行数变化\n",
        "    if merge_type == 'left':\n",
        "        if len(df_result) == len(df_left):\n",
        "            print(\"✅ 行数正常（一对一匹配）\")\n",
        "        elif len(df_result) > len(df_left):\n",
        "            print(f\"⚠️ 行数增加了 {len(df_result) - len(df_left)} 行（存在一对多关系）\")\n",
        "        else:\n",
        "            print(f\"❌ 行数减少了 {len(df_left) - len(df_result)} 行（异常！）\")\n",
        "    \n",
        "    # 检查新增的NULL\n",
        "    new_nulls = df_result.isnull().sum() - df_left.isnull().sum().reindex(df_result.columns, fill_value=0)\n",
        "    new_nulls = new_nulls[new_nulls > 0]\n",
        "    if len(new_nulls) > 0:\n",
        "        print(f\"\\n⚠️ 合并后新增的NULL值:\")\n",
        "        print(new_nulls)\n",
        "    else:\n",
        "        print(\"\\n✅ 无新增NULL值\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# 使用示例\n",
        "# validate_merge(df_main, df_dim, df_result, 'key', 'left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. ⚠️ 数据泄露警示\n",
        "\n",
        "**数据泄露（Data Leakage）** 是数模比赛中最容易犯的错误，会导致模型在测试集上表现极好，但实际预测完全失效。\n",
        "\n",
        "## 常见泄露场景\n",
        "\n",
        "| 场景 | 错误做法 | 正确做法 |\n",
        "|------|----------|---------|\n",
        "| **时序特征** | 用 `shift(-1)` 获取\"下一期\"数据 | 用 `shift(1)` 获取\"上一期\"数据 |\n",
        "| **滚动特征** | `rolling(3).mean()` 包含当期 | `rolling(3).mean().shift(1)` 排除当期 |\n",
        "| **标准化** | 用全部数据fit_transform | 只用训练集fit，测试集用同样的scaler transform |\n",
        "| **目标编码** | 用包含测试集的统计值 | 只用训练集计算统计值 |\n",
        "| **未来信息** | 使用预测时刻之后的数据 | 严格只使用预测时刻之前的数据 |\n",
        "\n",
        "## 自检方法\n",
        "\n",
        "```python\n",
        "# 假设你要预测2028年的数据\n",
        "# 检查：你的特征在2024年时是否就已经可以计算出来？\n",
        "\n",
        "# ❌ 错误：使用了2028年的信息\n",
        "df['avg_medals'] = df.groupby('country')['medals'].transform('mean')  # 包含2028\n",
        "\n",
        "# ✅ 正确：只使用2024年及之前的信息\n",
        "df['avg_medals'] = df.groupby('country')['medals'].transform(\n",
        "    lambda x: x.shift(1).expanding().mean()  # 只用历史数据\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. 总结\n",
        "\n",
        "## 🎯 数据预处理的核心原则\n",
        "\n",
        "1. **理解业务优先**：每一步处理都要有业务逻辑支撑，不是机械操作\n",
        "2. **可解释性**：用Markdown记录每个决策的原因，方便复盘和论文撰写\n",
        "3. **可复现性**：代码要清晰，别人（或未来的自己）能重新运行\n",
        "4. **验证验证再验证**：每次合并、变换后都要检查结果是否符合预期\n",
        "5. **警惕数据泄露**：时序问题尤其注意，严格区分\"已知\"和\"未知\"信息\n",
        "\n",
        "## 📚 推荐工作流程\n",
        "\n",
        "```\n",
        "拿到数据\n",
        "   ↓\n",
        "复制【万能Prompt】发给AI，附上数据文件\n",
        "   ↓\n",
        "AI生成定制化的预处理Notebook\n",
        "   ↓\n",
        "运行代码，观察输出\n",
        "   ↓\n",
        "对照【检查清单】确认没有遗漏\n",
        "   ↓\n",
        "需要补充？从【代码片段】中复制\n",
        "   ↓\n",
        "保存处理后的数据，进入建模阶段\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**祝你比赛顺利！🏆**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
